{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QVoxPKJz1WqX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "RK3uGmGBloGZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWW0Nja2lxFZ",
        "outputId": "ac57c2db-d1fa-4896-ec76-f69c6ae6f3c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Set paths for the original dataset and new directories\n",
        "base_dir = '/content/drive/MyDrive/Dataset'\n",
        "train_dir = '/content/drive/MyDrive/Dataset/train'\n",
        "val_dir = '/content/drive/MyDrive/Dataset/val'\n",
        "\n",
        "# Create directories for train and val splits\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(train_dir, 'eczema'), exist_ok=True)\n",
        "os.makedirs(os.path.join(train_dir, 'normal'), exist_ok=True)\n",
        "os.makedirs(os.path.join(val_dir, 'eczema'), exist_ok=True)\n",
        "os.makedirs(os.path.join(val_dir, 'normal'), exist_ok=True)\n",
        "\n",
        "# Function to split images into train/val\n",
        "def split_data(source_dir, train_dir, val_dir, val_size=0.2):\n",
        "    for category in ['eczema', 'normal']:\n",
        "        category_dir = os.path.join(source_dir, category)\n",
        "        files = os.listdir(category_dir)\n",
        "        random.shuffle(files)\n",
        "        val_count = int(len(files) * val_size)\n",
        "\n",
        "        # Move files to train and val directories\n",
        "        for file in files[val_count:]:\n",
        "            shutil.move(os.path.join(category_dir, file), os.path.join(train_dir, category, file))\n",
        "        for file in files[:val_count]:\n",
        "            shutil.move(os.path.join(category_dir, file), os.path.join(val_dir, category, file))\n",
        "\n",
        "# Split the data into train and val\n",
        "split_data(base_dir, train_dir, val_dir)\n"
      ],
      "metadata": {
        "id": "zSQT6y6OmEYk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for training and validation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(150),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(40),\n",
        "    transforms.RandomAffine(0, shear=0.2, scale=(0.8, 1.2)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet normalization\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((150, 150)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet normalization\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = ImageFolder(train_dir, transform=train_transform)\n",
        "val_dataset = ImageFolder(val_dir, transform=val_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load pre-trained MobileNetV2 and modify it\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)  # Binary classification\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X23K8koYm3O2",
        "outputId": "f76e59ed-058b-4bf9-bc1f-098154c142db"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 99.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers except the final classifier\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.float().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dataset)\n",
        "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.float().to(device)\n",
        "\n",
        "                outputs = model(inputs).squeeze()\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                preds = torch.sigmoid(outputs) > 0.5\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            val_loss = val_loss / len(val_dataset)\n",
        "            val_acc = val_corrects.double() / len(val_dataset)\n",
        "\n",
        "            print(f'Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.4f}')\n"
      ],
      "metadata": {
        "id": "ozRZ3qDonB2e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n",
        "\n",
        "# Save the model\n",
        "model_save_path = '/content/drive/MyDrive/eczema_model.pth'  # Save in your Google Drive\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "# Load the model for inference\n",
        "model_load_path = '/content/drive/MyDrive/eczema_model.pth'  # Load from your Google Drive\n",
        "model.load_state_dict(torch.load(model_load_path))\n",
        "\n",
        "# Function to classify images in a directory\n",
        "def classify_images_in_directory(directory_path, model, device):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((150, 150)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    for root, _, files in os.walk(directory_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
        "                img_path = os.path.join(root, file)\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    output = model(img_tensor).squeeze()\n",
        "                    prediction = torch.sigmoid(output).item()\n",
        "\n",
        "                if prediction < 0.5:\n",
        "                    print(f\"{file}: Predicted class = Eczema\")\n",
        "                else:\n",
        "                    print(f\"{file}: Predicted class = Normal\")\n",
        "\n",
        "# Test on unseen images\n",
        "unseen_images_dir = '/unseen.jpg'\n",
        "classify_images_in_directory(unseen_images_dir, model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmNobyernUdk",
        "outputId": "cdef4647-d013-41f8-d749-340123d1eacf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3688, Acc: 0.8549\n",
            "Validation Loss: 0.3720, Validation Acc: 0.8611\n",
            "Epoch 2/10, Loss: 0.3659, Acc: 0.8571\n",
            "Validation Loss: 0.3667, Validation Acc: 0.8611\n",
            "Epoch 3/10, Loss: 0.3621, Acc: 0.8549\n",
            "Validation Loss: 0.3651, Validation Acc: 0.8611\n",
            "Epoch 4/10, Loss: 0.3546, Acc: 0.8549\n",
            "Validation Loss: 0.3615, Validation Acc: 0.8611\n",
            "Epoch 5/10, Loss: 0.3368, Acc: 0.8549\n",
            "Validation Loss: 0.3576, Validation Acc: 0.8611\n",
            "Epoch 6/10, Loss: 0.3348, Acc: 0.8549\n",
            "Validation Loss: 0.3540, Validation Acc: 0.8611\n",
            "Epoch 7/10, Loss: 0.3368, Acc: 0.8549\n",
            "Validation Loss: 0.3479, Validation Acc: 0.8611\n",
            "Epoch 8/10, Loss: 0.3261, Acc: 0.8594\n",
            "Validation Loss: 0.3498, Validation Acc: 0.8611\n",
            "Epoch 9/10, Loss: 0.3218, Acc: 0.8639\n",
            "Validation Loss: 0.3421, Validation Acc: 0.8611\n",
            "Epoch 10/10, Loss: 0.3048, Acc: 0.8617\n",
            "Validation Loss: 0.3386, Validation Acc: 0.8611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-f14bcf35c274>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_load_path))\n"
          ]
        }
      ]
    }
  ]
}